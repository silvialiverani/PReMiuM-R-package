
\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath,natbib}
\usepackage{times,verbatim}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in


%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}
\SweaveOpts{concordance=TRUE}

%------------------------------------------------------------
\title{PReMiuM manual}
%------------------------------------------------------------
\author{Silvia Liverani}
%\date{}

\SweaveOpts{highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE}
\SweaveOpts{prefix.string=Fig}


\maketitle
\tableofcontents

%-------------------------------------------
\section{Introduction}
%--------------------------------------------
First of all, we load the R package PReMiuM and set a seed to ensure every run gives the same results in this manual. 

<<test1>>=
library(PReMiuM)
set.seed(1234)
@

%-------------------------------------------
\section{Generating data}
%--------------------------------------------

PReMiuM includes a few functions to generate data. The data is generated by the function generateSampleDataFile(). The input of the function is a list of parameters. In the example below we generated a Bernoulli (ie. binary) distributed outcome and discrete covariates. The exact parameters of the data generating functions are obtained by writing the command in R. Some of the values returned in a list by this function are shown below: cluster sizes; the parameters for cluster 1 (value of $\theta_1$ as well as the probabilities for the categories of the 5 discrete covariates - they all 3 levels); the vector of fixed effect coefficients $\beta$ and the probability with which missing values are to be inputted into the dataset.

<<test1>>=
clusSummaryBernoulliDiscrete()$clusterSizes
clusSummaryBernoulliDiscrete()$clusterData[[1]]
clusSummaryBernoulliDiscrete()$fixedEffectsCoeffs
clusSummaryBernoulliDiscrete()$missingDataProb
@

Other functions to generate data from other models (the names of the functions should be self explanatory). 

<<test2, eval=FALSE, echo=TRUE>>=
clusSummaryBernoulliDiscrete()
clusSummaryBernoulliNormal
clusSummaryBernoulliDiscreteSmall()
clusSummaryBinomialNormal()
clusSummaryCategoricalDiscrete()
clusSummaryNormalDiscrete()
clusSummaryNormalNormal()
clusSummaryNormalNormalSpatial()
clusSummaryPoissonDiscrete()
clusSummaryPoissonNormal()
clusSummaryPoissonNormalSpatial()
clusSummaryVarSelectBernoulliDiscrete()
clusSummaryBernoulliMixed()
clusSummaryWeibullDiscrete()
clusSummaryQuantileNormal()
@

The data is then generated as follows. The dataset is stored in a list environment with the following output obtained when running it. 

<<test3>>=
inputs <- generateSampleDataFile(clusSummaryBernoulliDiscrete())
head(inputs$inputData)
inputs$covNames
inputs$xModel
inputs$yModel
inputs$nCovariates
inputs$fixedEffectNames
inputs$outcomeT
@

%-------------------------------------------
\section{Running PReMiuM}
%--------------------------------------------
We can now run profile regression using the data simulated above. The command profRegr(), which runs profile regression, is effectively running an MCMC. For that reason, it is recommended that it runs over many thousands, or tens of thousands, or more iterations. However, for simplicity and speed we will only run the MCMC for very few iterations in this manual. The burn in period is determined by the option nBurn and the number of iterations following the burn-in period is determined by the option nSweeps. 

The output of this function is very large, so it is all saved in external files which are created when the command is run. For this reason it is important to know in which folder the command is to be run, to know where to find the output files. The files will all be named \verb|xyz_content.txt| where \verb|xyz| is the name of the profile regression run and can be set by providing option output (which in the example below is \verb|output|) and \verb|content| is the content of the file (explained in more detail in the following section).

<<test1>>=
runInfoObj<-profRegr(yModel=inputs$yModel, 
    xModel=inputs$xModel, nSweeps=100, nClusInit=15,
    nBurn=300, data=inputs$inputData, output="output", 
    covNames = inputs$covNames, 
    fixedEffectsNames = inputs$fixedEffectNames, seed=12345)
@

%-------------------------------------------
\subsection{Outputs of profRegr()}
%--------------------------------------------
Several files are created when \verb|profRegr()| is run. We discuss the most important files below. 

\begin{description}
\item[Data input] Before running the MCMC, profRegr() reads the inputs file and creates a file with the information needed to run the MCMC. The first 10 lines of this file are printed below. 
<<test1>>=
readLines("output_input.txt",15)
@

\item[Number of clusters] This file is of length equal to nSweeps (the number of iterations of the MCMC after the burn in period). Each line corresponds to an iteration and it includes a signle number, which is the number of clusters in that iteration. 
<<test1>>=
nClustersSweep1<-read.table("output_nClusters.txt")[1,1]
nClustersSweep1
@

\item[Number of observations in each cluster] Each line of this file corresponds to one iteration of the MCMC. Each line gives the size of the clusters. The last number is the total number of observations clustered (ie the sum of all preceeding numbers in the line). The first line of this file is shown below.  
<<test1>>=
as.numeric(strsplit(readLines("output_nMembers.txt",1)," ")[[1]])
@

\item[Log] This file includes several details about the MCMC run, including the parameters used, the MCMC samplers used, their acceptance rate, etc. See below. 

<<test1>>=
readLines("output_log.txt")
@

\item[Cluster-specific parameters] There are several cluster-specific parameters in the model. Parameters that are cluster-specific are those that have a different value for each cluster. As the number of clusters changes from one sweep to the next, the number of these parameters changes from one sweep to the next. Each line in this file corresponds to an iteration of the MCMC, but each line has a different number of elements. The file for $\theta$ has one parameter for each cluster. The file for $\phi$ has the 
<<test1>>=
as.numeric(strsplit(readLines("output_theta.txt",1)," ")[[1]])
phiSweep1<-as.numeric(strsplit(readLines("output_phi.txt",1)," ")[[1]])
head(phiSweep1)
@

In the case of the cluster-specific vectors of parameters, such as $\phi$, the data is printed over the clusters, then over the levels and then the covariates. So, for example, first we have the values of $\phi$ for level 1 of variable 1 in cluster 1. Then we have the value of level 1 of variable 1 in cluster 2, and so on. When all the values for all the clusters for level 1 of variables 1 are printed, the value of level 2 for variable 1 is printed to file, from the first to the last cluster. In the example below, all the values of $\phi$ are printed for cluster 1. In particular, the first three values are the posterior probabilities of level 1, level 2 and level 3 for variable 1. As expected, these three numbers sum up to 1. 

<<test1>>=
phiSweep1[seq(1,length(phiSweep1),nClustersSweep1)]
@

\item[Global parameters] These are the other parameters of the model. Each line of these files corresponds to an iteration of the MCMC. The parameter $\beta$ is for the fixed effects. The number of elements in this file is fixed and it corresponds to the number of fixed effects included in the model. The parameter $\alpha$ is the MCMC sample from the posterior distribution of $\alpha$, the parameter of the Dirichlet process, and it is present only if $\alpha$ is random (this is the default in profRegr; it can alternatively be set equal to a specific value).
<<test1>>=
as.numeric(strsplit(readLines("output_beta.txt",1)," ")[[1]])
as.numeric(strsplit(readLines("output_alpha.txt",1)," ")[[1]])
@

\end{description}


%-------------------------------------------
\section{Postprocessing}
%--------------------------------------------
The output of \verb|PReMiuM| is very rich, so postprocessing is necessary to learn about the clustering structure, for example. There are three main functions for postprocessing. 

<<test1>>=
dissimObj<-calcDissimilarityMatrix(runInfoObj)
clusObj<-calcOptimalClustering(dissimObj,maxNClusters = 7)
riskProfileObj<-calcAvgRiskAndProfile(clusObj)
@

\begin{description}
\item[Computation of dissimilarity matrix] First of all a dissimilarity matrix is computed. This is a matrix which is $1-S$, where $S$ is a similarity matrix. See \cite{Molitor10} Molitor et al (2010) for more details.   
\item[Computation of optimal clustering] This function computes the optimal clustering based on the dissimilarity matrix computed in the previous step. It computes the optimal clustering by applying partitioning around medoids (PAM) to the dissimilarity matrix. Partitioning around medoids is a method that requires the number of clusters to be preset, so this function makes multiple attempts to fitting PAM with different number of clusters. In the example here I have set the maximum number of clusters to 7. 
\item[Computation of risks and profiles] This function computes the risks and the profiles for all covariates and all clusters. In order to do so, it looks up which cluster each observation belongs to in the final clustering. Then to compute the risks, for each MCMC iteration it computes a $\theta$ value, which is the average of all the $\theta$ values of the clusters which included the observations in each final cluster.  
\end{description}

%-------------------------------------------
\section{Plot}
%--------------------------------------------
The plot in Figure \ref{fig:summary} can be produced by running the following command. 
<<test1>>=
clusterOrderObj<-plotRiskProfile(riskProfileObj,"summary.png")
@
The data for the boxplots of the outcome are available in \verb|riskProfileObj$risk| while the data for the boxplots of the covariates are available in \verb|riskProfileObj$profile|.

\begin{figure}[ht]
\centering
\includegraphics[height=9cm]{"summary"}
\caption{This is summary.png, the plot created by running plotRiskProfile().\label{fig:summary}}
\end{figure}

%------------------------------------
\section{Plots for known-truth clusterings}
%-------------------------------------
The simulated data is generated for 5 clusters of 200 observations each. Let us explore some plots for the output for cases when the true clustering of the observations is known. Say that the known-truth partition is given by the following.

<<test1>>=
known<-c(rep("A",600),rep("B",600),rep("C",300),rep("D",500),rep("E",1000))
@

The data is generated as follows. 

<<test1>>=
inputs <- generateSampleDataFile(clusSummaryNormalDiscrete())
@

Then we run profile regression.

<<test1>>=
runInfoObj<-profRegr(yModel=inputs$yModel, 
    xModel=inputs$xModel, nSweeps=100, nClusInit=15,
    nBurn=300, data=inputs$inputData, output="output", 
    covNames = inputs$covNames, 
    fixedEffectsNames = inputs$fixedEffectNames, seed=12345)
dissimObj<-calcDissimilarityMatrix(runInfoObj)
clusObj<-calcOptimalClustering(dissimObj,maxNClusters = 7)
riskProfileObj<-calcAvgRiskAndProfile(clusObj)
clusterOrderObj<-plotRiskProfile(riskProfileObj,"summary2.png")
@
The optimal partition found by profile regression is given by the following command. 
<<test1>>=
optAlloc<-clusObj$clustering
@

We now want to produce a few plots to compare the known truth to the clustering obtained by profile regression. 

The first plot is showing how the distribution of the outcome for each true cluster is spread over the clusters identified by profile regression. 

<<test2,fig=TRUE>>=
library(ggplot2)    
tmp_boxplot<-data.frame(opt=as.factor(optAlloc),outcome=inputs$inputData$outcome,known=as.factor(known))
p <- ggplot(tmp_boxplot, aes(x=known, y=outcome, fill=opt)) + 
  geom_violin()+
  labs(title="",x="Known Truth", y = "outcome") +
  facet_grid(~known,scales='free',space='free') + 
  guides(fill=guide_legend(title="Clusters")) +
  theme(strip.text.x = element_blank(), strip.background = element_blank()) 
# Use brewer color palettes
p+scale_fill_brewer(palette="Set1")
@

We can also do this plot replacing the distribution of observed outcome with the posterior distribution of $\theta$. 

<<test3,fig=TRUE>>=
# Construct the number of clusters file name
nClustersFileName<-file.path(runInfoObj$directoryPath,paste(runInfoObj$fileStem,'_nClusters.txt',sep=''))
nClustersFile<-file(nClustersFileName,open="r")
  
# Construct the allocation file name
zFileName <- file.path(runInfoObj$directoryPath,paste(runInfoObj$fileStem,'_z.txt',sep=''))
zFile<-file(zFileName,open="r")
  
# Construct the theta file name
thetaFileName <- file.path(runInfoObj$directoryPath,paste(runInfoObj$fileStem,'_theta.txt',sep=''))
thetaFile<-file(thetaFileName,open="r")

firstLine<-ifelse(runInfoObj$reportBurnIn,runInfoObj$nBurn/runInfoObj$nFilter+2,1)
lastLine<-(runInfoObj$nSweeps+ifelse(runInfoObj$reportBurnIn,runInfoObj$nBurn+1,0))/runInfoObj$nFilter
nSamples<-lastLine-firstLine+1

thetaByObs<-matrix(NA,ncol=runInfoObj$nSubjects,nrow=runInfoObj$nSweeps)

for(sweep in firstLine:lastLine){
    if(sweep==firstLine){
      skipVal<-firstLine-1
    }else{
      skipVal<-0
    }

    currMaxNClusters<-scan(nClustersFile,what=integer(),skip=skipVal,n=1,quiet=T)

    # Get the current allocation data for this sweep
    currZ<-scan(zFile,what=integer(),skip=skipVal,n=runInfoObj$nSubjects+runInfoObj$nPredictSubjects,quiet=T)
    currZ<-1+currZ
    
    # Get the risk data corresponding to this sweep
    currThetaVector<-scan(thetaFile,what=double(),skip=skipVal,n=currMaxNClusters,quiet=T)
    currTheta<-matrix(currThetaVector,ncol=1,byrow=T)

    for (i in 1:runInfoObj$nSubjects){
      thetaByObs[sweep,i]<-currTheta[currZ[i]]
    }
}

tmp_boxplot2<-data.frame(opt=vector(),outcome=vector(),known=vector())

for (c in 1:max(optAlloc)){
  for (k in names(table(known))){
    tmp_index<-which(optAlloc==c & known==k)
    tmp_length<-length(tmp_index)
    if (tmp_length>1){
      tmp_value<-apply(thetaByObs[,tmp_index],1,mean)
      tmp_boxplot3<-data.frame(opt=rep(c,runInfoObj$nSweeps),
                               outcome=tmp_value,known=rep(k,runInfoObj$nSweeps))
      tmp_boxplot2<-rbind(tmp_boxplot2,tmp_boxplot3)
    }
    if (tmp_length==1){
      tmp_boxplot3<-data.frame(opt=rep(c,runInfoObj$nSweeps),outcome=thetaByObs[,tmp_index],known=rep(k,runInfoObj$nSweeps))
      tmp_boxplot2<-rbind(tmp_boxplot2,tmp_boxplot3)
    }
  }
}

tmp_boxplot2$opt<-as.factor(tmp_boxplot2$opt)
tmp_boxplot2$known<-as.factor(tmp_boxplot2$known)

p <- ggplot(tmp_boxplot2, aes(x=known, y=outcome, fill=opt)) + 
  geom_violin()+
  labs(title="",x="Known Truth", y = "outcome") +
  facet_grid(~known,scales='free',space='free') + 
  guides(fill=guide_legend(title="Clusters")) +
  theme(strip.text.x = element_blank(), strip.background = element_blank()) 
# Use brewer color palettes
p#+scale_fill_brewer(palette="Set1")

# close all files
close(zFile)
close(nClustersFile)
close(thetaFile)
@

Then we want to do some composite colour bars. 
<<test1>>=
inputs <- generateSampleDataFile(clusSummaryNormalNormal())
runInfoObj<-profRegr(yModel=inputs$yModel, 
    xModel=inputs$xModel, nSweeps=100, nClusInit=15,
    nBurn=300, data=inputs$inputData, output="output", 
    covNames = inputs$covNames, 
    seed=12345)
dissimObj<-calcDissimilarityMatrix(runInfoObj)
clusObj<-calcOptimalClustering(dissimObj,maxNClusters = 7)
riskProfileObj<-calcAvgRiskAndProfile(clusObj)
clusterOrderObj<-plotRiskProfile(riskProfileObj,"summary3.png")
@

Then we make a composite colour bar plot. 
<<test1,fig=TRUE>>=
profile<-riskProfileObj$profile
meanSortIndex<-clusterOrderObj

muArray<-data.frame(covariateNumber=vector(),clusterNumber=vector(),colourPlot=vector())

for(j in 1:runInfoObj$nCovariates){
  # Compute the means
  muMat<-profile[,meanSortIndex,j]
  muMeans<-apply(muMat,2,mean)
  muMean<-sum(muMeans*clusObj$clusterSizes)/sum(clusObj$clusterSizes)
  muLower<-apply(muMat,2,quantile,0.05)
  muUpper<-apply(muMat,2,quantile,0.95)

    # Get the plot colors
  muColor<-ifelse(muLower>rep(muMean,length(muLower)),"high",
                ifelse(muUpper<rep(muMean,length(muUpper)),"low","avg"))
  muArrayNew<-data.frame(covariateNumber=rep(j,clusObj$nClusters),
                         clusterNumber=c(1:clusObj$nClusters),
                         colourPlot=muColor)
  muArray<-rbind(muArray,muArrayNew)  
}
# THIS IS NOT WORKING YET!!

ggplot(muArray, aes(x=clusterNumber, y=covariateNumber, fill=colourPlot)) +
  geom_col(stat="identity", colour="white") 

@

%------------------------------------
\section{Predictions}
%-------------------------------------
Profile regression can be used to compute the posterior predictive distribution of the outcome for given values of the covariates. For example, let us simulate the data as follows. 
<<predict1>>=
inputs <- generateSampleDataFile(clusSummaryBernoulliDiscrete())
head(inputs$inputData)
@
We can then prepare a data frame which contains the covariate values which we are interested in computing a prediction of the outcome for. The data frame must have as many columns as the number of covariates in the original dataset, and it may contain \verb|NA| for covariate values which are unknown. The number of rows of the dataset corresponds to the number of predictive profiles that we are interested in computing. The columns of the data frame must have names that match the covariates in the original dataset.  
<<predict1>>=
preds<-data.frame(matrix(c(
2, 2, 2, 2, 2,
0, 0, NA, 0, 0),ncol=5,byrow=TRUE))
colnames(preds)<-names(inputs$inputData)[2:(inputs$nCovariates+1)]
preds
@
Then we can include this dataset into the function \verb|profRegr| using the option \verb|predict| and run the MCMC and the postprocessing functions.  
<<predict1,results=hide>>=
runInfoObj<-profRegr(yModel=inputs$yModel, xModel=inputs$xModel, 
 nSweeps=10000, nBurn=10000, data=inputs$inputData, output="output", 
 covNames=inputs$covNames,predict=preds,
 fixedEffectsNames = inputs$fixedEffectNames)        
dissimObj <- calcDissimilarityMatrix(runInfoObj)
clusObj <- calcOptimalClustering(dissimObj)
riskProfileObj <- calcAvgRiskAndProfile(clusObj)
@
The function \verb|calcPredictions| extracts the sample from the posterior predictive distribution for the predictive profiles. Below, the first 10 samples from the MCMC are shown for the two predictive profiles selected for this example. 
<<predict1,results=hide>>=
predictions <- calcPredictions(riskProfileObj,fullSweepPredictions=TRUE,fullSweepLogOR=TRUE)
predictions$predictedYPerSweep[1:10,,1]
@
The following command plots the posterior predictive distribution for each of the predictive profiles in the matrix \verb|predictions$predictedYPerSweep|. See Figure \ref{fig:predict}. 
<<predict1,results=hide>>=
plotPredictions(outfile="predictiveDensity.pdf",runInfoObj=runInfoObj,
 predictions=predictions,logOR=TRUE)
@

\begin{figure}[ht]
\centering
\includegraphics[height=7cm,page=1]{"predictiveDensity"}
\includegraphics[height=7cm,page=1]{"predictiveDensity"}
\caption{These are the posterior predictive distributions for the two predictive profiles, saved in `predictiveDensity.pdf' using the function plotPredictions.\label{fig:predict}}
\end{figure}

%------------------------------------
\section{SessionInfo}
%-------------------------------------

This file was compiled using a computer with the following packages: 

<<sessionInfo>>=
sessionInfo()
@ 

\bibliographystyle{chicago}
\bibliography{biblio}
\end{document}

